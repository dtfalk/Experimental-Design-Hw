
---
title: "Homework 2"
author: "David Falk"
date: "`r Sys.time()`"
output: 
  html_document: 
    toc: true
---
<!---
In order to check that all the necessary packages are installed, Click the File > New File > R Markdown button and keep all of the options as is. Save the document somewhere, and, while that new document is open, click the knit button. If a window with some plots shows up, everything is installed correctly. If RStudio prompts you to install any packages, you should install them.
-->

<!---
Please save this file as Homework2_lastname_firstname.Rmd

Be sure to update the author field in the block above.

While you are working on your homework, you can use the green arrows to run a "chunk" of code. In order to produce the final document, use the knit button. You will turn in both the Rmd file and the knitted html file, which will appear next to wherever you saved the Rmd file if there are no errors.

Beware, if you run chunks out of order or reuse variable names your code may produce different results when it is run again from top to bottom. Before you knit your document, it is a good idea to run all the chunks in order from top to bottom in a clean environment. Use the broom button in the environment pane to clear your variables and click the Session > Restart R and Run All Chunks.

If you ever want to reference the documentation about a function, go to the console below and type ?function_name.
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
# Can set your local path to control exactly where this saves. Otherwise, will save in directory with the Rmd file
# knitr::opts_knit$set(root.dir = "/home/expdes/outputs/")  

library(tidyverse)  # hub of many packages
set.seed(1)  # reproducible results
theme_set(theme_minimal())  # nice-looking plots

```

# Part 0: Importing Data  
Import `participant_level_data` from Canvas.  
```{r}
participant_level_data <- read.csv(file.path("data", "participant_level_data.csv"))
```

   
# Part 1: Normality and t-tests  
Now we can test out one of our main questions: Does encoding strategy impact memory performance?  
  
## Question 1:  
Plot one histogram each of `true_recall_proportion` and `false_recall_proportion` across all participants.
```{r}
# Sources
#   1) https://www.datamentor.io/r-programming/histogram

# Get the recall proportions
true_recall_data <- participant_level_data$true_recall_proportion
false_recall_data <- participant_level_data$false_recall_proportion


# HISTOGRAMS
# ================================

# True Recall Proportion
# -------------------
hist(true_recall_data,
     main = "True Recall Proportions Across All Participants", 
     xlab = "True Recall Proportion", 
     ylab = "Count",
     xlim = c(0,1),
     ylim = c(0,8),
     col  = "darkolivegreen4"
     )
# -------------------

# False Recall Proportion
# -------------------
hist(false_recall_data,
     main = "False Recall Proportions Across All Participants", 
     xlab = "False Recall Proportion", 
     ylab = "Count",
     xlim = c(0,1),
     ylim = c(0,8),
     col  = "brown1"
     ) 
```

Briefly discuss: What shape do these distributions take? Are there any particularly noticeable features about these distributions?    
 
The histogram for the true recall proportions appears normal. There may be too little data to tell but it appears to have a large, normal-looking bump around 0.5 to 0.7 and then the graph tapers off on both the lefts and the right. On the left tail there is a potential outlier around 0.1, but it is hard to say if this is of signifigance just by eyeballing the graph.

The historgram for the false recall proportions appears to be either uniform or heavier closer to 0 and tapering off towards thr right. It is hard to tell which it would be without more data or a more advanced statistical analysis (Kolomgrov-Smirnov?). If we had more data then I would be interested in seeing whether that dip around 0.4-0.7 fills in and everything evens out or whether the areas around 0.0-0.4 proceed to fill out.

In summary, the two distributions appear to have different shapes. True recall looks more normal and false recall looks either uniform or skewed in some way.

  
## Question 2:  
Write an R function (called "**ttest**" ← function(samp1,samp2)) that calculates the t-statistic for an **independent samples t-test** from scratch. Make sure that if the mean of sample 1 is greater than the mean of sample 2, the t statistic is positive (and negative if the mean of sample 1 is less than the mean of sample 2). Show how you would also calculate these values using a built-in R function.
```{r}
# Sources
#   1) https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test
#   2) https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/unname

# Handmade function
ttest <- function(samp1, samp2) {
  
  # Get n for each sample
  n_1 <- length(samp1)
  n_2 <- length(samp2)
  
  # Get mean of each sample
  mean_1 <- mean(samp1)
  mean_2 <- mean(samp2)
  
  # Get standard deviation of each sample
  std_1 <- sd(samp1)
  std_2 <- sd(samp2)
  
  # Compute Pooled Standard Deviation
  pooled_std_numerator   <- ((n_1 - 1) * (std_1 ** 2)) + ((n_2 - 1) * (std_2 ** 2)) 
  pooled_std_denominator <- n_1 + n_2 - 2
  pooled_std <- sqrt(pooled_std_numerator / pooled_std_denominator)
  
  # Compute the t-statistic
  t_stat_numerator   <- mean_1 - mean_2 
  t_stat_denominator <- pooled_std * sqrt((1 / n_1) + (1 / n_2))
  t_statistic <- t_stat_numerator / t_stat_denominator
  
  return(t_statistic)
}
```

a. Use your “**ttest**” function to compare `true_recall_proportion` for verbal versus visual participants. Output the resulting t-statistic.  
```{r}

# Get verbal and visual participant data
verbal_true_recall_data <- participant_level_data[participant_level_data$encoding_modality == "verbal",]$true_recall_proportion

visual_true_recall_data <- participant_level_data[participant_level_data$encoding_modality == "visual",]$true_recall_proportion

# Calculate t statistic
indep_t_stat_modality <- ttest(verbal_true_recall_data, visual_true_recall_data)

# Print the result
cat(sprintf("(Encoding Modality) Handwritten Indep. Samp. T-Statistic: %.6f", indep_t_stat_modality))
```

b. Use your “**ttest**” function to compare `true_recall_proportion` for shallow versus deep strategy participants. Output the resulting t-statistic.  
```{r}

# Get shallow and deep participant data
shallow_true_recall_data <- participant_level_data[participant_level_data$encoding_depth == "shallow",]$true_recall_proportion

deep_true_recall_data <- participant_level_data[participant_level_data$encoding_depth == "deep",]$true_recall_proportion

# Calculate t statistic
indep_t_stat_depth <- ttest(shallow_true_recall_data, deep_true_recall_data)

# Print the result
cat(sprintf("(Encoding Depth) Handwritten Indep. Samp. T-Statistic: %.6f", indep_t_stat_depth))
```

Show how you would also calculate it using a built-in R function.   
```{r}

# Encoding modality via built-in function
indep_t_stat_modality_builtin <- unname(t.test(verbal_true_recall_data, visual_true_recall_data, var.equal = TRUE)$statistic)
cat(sprintf("(Encoding Modality) Built-in Indep. Samp. T-Statistic:  %.6f\n", indep_t_stat_modality_builtin))

# Encoding depth via built-in function
indep_t_stat_depth_builtin <- unname(t.test(shallow_true_recall_data, deep_true_recall_data, var.equal = TRUE)$statistic)
cat(sprintf("(Encoding Depth)    Built-in Indep. Samp. T-Statistic: %.6f", indep_t_stat_depth_builtin))

```


  
## Question 3:  
Write an R function called "**ttest_paired**" that calculates the t-statistic for a **paired samples t-test** from scratch. Make sure that if the mean of sample 1 is greater than the mean of sample 2, the t-statistic is positive.  
```{r}
# Sources
#   1) https://datascienceparichay.com/article/subtract-two-vectors-in-r/

# Handmade paired t test function
ttest_paired <- function(samp1, samp2) {
  
  # Get a vector representing paired differences
  differences <- samp1 - samp2
  
  # Get mean, std and n of the paired differences
  mean_diff <- mean(differences)
  std_diff <- sd(differences)
  n <- length(differences)
  
  # Calculate t statistic
  t_statistic <- mean_diff / (std_diff / sqrt(n))
  
  return(t_statistic)
}
```

Use your "**ttest_paired**" function to compare `true_recall_proportion` for verbal versus visual participants. Output the resulting t-statistic. For this question, only use the first 12 participants in each condition (remove ids 862003, TgA3wccPQM9Has7)  
```{r}
# Ids to filter out for modality
modality_ids_to_filter <- c("862003", "TgA3wccPQM9Has7")

# Filter out specified IDs
no_ids_modality <- participant_level_data[!(participant_level_data$participant %in% modality_ids_to_filter),]

# Get verbal vector
verbal_true_recall_data_paired <-
  no_ids_modality$true_recall_proportion[
    no_ids_modality$encoding_modality == "verbal"][1:12]

# Get visual vector
visual_true_recall_data_paired <-
  no_ids_modality$true_recall_proportion[
    no_ids_modality$encoding_modality == "visual"][1:12]

# Get paired T-statistic
paired_t_stat_modality_handwritten <- ttest_paired(verbal_true_recall_data_paired, visual_true_recall_data_paired)

# Print the result
cat(sprintf("(Encoding Modality) Handwritten Paired Samples T-Statistic: %.6f", paired_t_stat_modality_handwritten))
```

Use your "**ttest_paired**" function to compare `true_recall_proportion` for shallow versus deep strategy participants. Output the resulting t-statistic. For this question, only use the first 11 participants in each condition (remove ids 662545, 665565, 697823, 862003)  
```{r}
# Ids to filter out for depth
depth_ids_to_filter <- c("662545", "665565", "697823", "862003")

# Filter out specified IDs
no_ids_depth <- participant_level_data[!(participant_level_data$participant %in% depth_ids_to_filter),]

# Get verbal vector
shallow_true_recall_data_paired <-
  no_ids_depth$true_recall_proportion[
    no_ids_depth$encoding_depth == "shallow"][1:11]

# Get visual vector
deep_true_recall_data_paired <-
  no_ids_depth$true_recall_proportion[
    no_ids_depth$encoding_depth == "deep"][1:11]

# Get paired T-statistic
paired_t_stat_depth_handwritten <- ttest_paired(shallow_true_recall_data_paired, deep_true_recall_data_paired)

# Print the result
cat(sprintf("(Encoding Depth) Handwritten Paired Samples T-Statistic: %.6f", paired_t_stat_depth_handwritten))
```

Show how you would also calculate it using a built-in R function.   
```{r}
# Sources
#   1) https://www.statology.org/paired-samples-t-test-r/

# Encoding modality via built-in function
paired_t_stat_modality_builtin <- unname(t.test(verbal_true_recall_data_paired, visual_true_recall_data_paired, paired = TRUE)$statistic)
cat(sprintf("(Encoding Modality) Built-in Paired Samp. T-Statistic:  %.6f\n", paired_t_stat_modality_builtin))

# Encoding depth via built-in function
paired_t_stat_depth_builtin <- unname(t.test(shallow_true_recall_data_paired, deep_true_recall_data_paired, paired = TRUE)$statistic)
cat(sprintf("(Encoding Depth)    Built-in Paired Samp. T-Statistic: %.6f", paired_t_stat_depth_builtin))

```


  
## Question 4: Write up  
a: Briefly discuss which is the more appropriate test to use, and why.   

The independent samples is more appropriate for this situation. 

The independent samples test is appropriate for comparing different groups. For example "visual" vs "verbal" encoding modality or "deep" vs "shallow" encoding depth. 

The paired samples test is inappropriate here because paired tests assume that there is paired data for each subject. Since each subject has EITHER a visual or verbal encoding modality and EITHER a deep or shallow encoding modality there is no meaningful way to compute paired differences. There is no paired data.

  
b: Get the p-value for that test using the R function `pt()`. Make sure to output this value.  
```{r}
# Sources
#   1) https://www.statology.org/working-with-the-student-t-distribution-in-r-dt-qt-pt-rt/

df_modality <- length(participant_level_data[participant_level_data$encoding_modality == "verbal",]$true_recall_proportion) +
  length(participant_level_data[participant_level_data$encoding_modality == "visual",]$true_recall_proportion) - 2

p_value_modality <- 2 * pt(abs(indep_t_stat_modality), df_modality, lower.tail = FALSE)

df_depth <- length(participant_level_data[participant_level_data$encoding_depth == "shallow",]$true_recall_proportion) +
  length(participant_level_data[participant_level_data$encoding_depth == "deep",]$true_recall_proportion) - 2

p_value_depth <- 2 * pt(abs(indep_t_stat_depth), df_depth, lower.tail = FALSE)

cat(sprintf("(Encoding Modality) P Value: %.6f\n", p_value_modality))
cat(sprintf("(Encoding Depth)    P Value: %.6f", p_value_depth))
```
  
Describe – what is the null hypothesis and the alternative hypothesis for each test, and what does the p-value tell us about either of these?  
*Please write your answer to the question in the lines below* 
Modality\n
--------------------------\n
Null Hypothesis: There is no difference in true recall performance between the visual and verbal groups.

Alternative Hypothesis:
There is a difference in true recall performance between the visual and verbal groups.

P-Value Interpretation:
The p-value represents the probability of seeing a difference in true recall performance for the encoding modality equal to or larger than the sample difference if you assume that the null hypothesis is true.
--------------------------\n


Depth\n
--------------------------\n
Null Hypothesis (Depth): There is no difference in true recall performance between the shallow and deep groups.

Alternative hypothesis:
There is a difference in true recall performance between the shallow and deep groups.
 
P-Value Interpretation:
The p-value represents the probability of seeing a difference in true recall performance for the encoding strategy equal to or larger than the sample difference if you assume that the null hypothesis is true.
--------------------------\n

c: Write up the t-test results for each test in the format of a sentence for a research paper.  

Modality: An independent samples t-test did not identify a signifcant difference between true recall performance between subjects visual vs verbal encoding modality conditions (df = 20 t = 0.286, p = 0.777).

Depth: An independent-samples t-test did not identify a statistically significant difference in true recall performance for subjects in the shallow vs deep encoding strategies, (df = 22, t = -0.666, p = 0.512).
  
## Question 5:  
Write a function (called "**cohend**" <- function(samp1, samp2)) to calculate the effect size (Cohen’s d) from scratch (taking the two samples as the input).  
```{r}
# Sources
#   1) https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/cohens-d/
#   2) https://www.statology.org/pooled-standard-deviation/

cohend <- function(samp1, samp2) {
  
  # Get mean for each sample
  mean1 <- mean(samp1)
  mean2 <- mean(samp2)
  
  # Get pooled standard deviation
  pooled_std_numerator <- ((length(samp1) - 1) * sd(samp1)^2) + ((length(samp2) - 1) * sd(samp2)^2)
  pooled_std_denominator <- length(samp1) + length(samp2) - 2
  pooled_std <- sqrt(pooled_std_numerator / pooled_std_denominator)
  
  # calculate cohen's d and return
  cohens_d <- (mean1 - mean2) / pooled_std
  return(cohens_d)
}
```

What is the Cohen’s d for the deep versus shallow strategy comparison? Output the value for Cohen’s d.  
```{r}
# Calculate Cohen's d using your function here

# Get deep participants' true recall data
deep_true_recall_data <-
  participant_level_data$true_recall_proportion[
    participant_level_data$encoding_depth == "deep"]

# Get shallow participants' true recall data
shallow_true_recall_data <-
  participant_level_data$true_recall_proportion[
    participant_level_data$encoding_depth == "shallow"]

# Calculate Cohen's d and print 
cohens_d_encoding_strategy <- cohend(deep_true_recall_data, shallow_true_recall_data)
cat(sprintf("Cohen's d for deep (samp1) vs shallow (samp2) encoding strategy: %.6f\n", cohens_d_encoding_strategy))

```

  
## Question 6:  
Re-write up the t-test results including effect size. Is this a small, medium, or large effect?  
An independent samples t-test was conducted to compare true recall performance between shallow and deep encoding strategies and did not reveal a statistically significant difference between groups, t(22) = -0.666, p = 0.512. The effect size was small (Cohen’s d = 0.26), suggesting that depth of encoding had, at most, only a small impact on true recall performance in this sample.
  
## Question 7:  
Using `power.t.test` in R, calculate the optimal sample size using the effect size from these data and a desired power level of 90%. Save the results of the power calculation to a variable called **power_res**.
```{r}
# Sources:
# 1) https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/power.t.test

# Calculate power_res
# (Maybe I am wrong but we can just re-scale pooled std
# to 1 and use Cohen's d directly?)
power_res <- power.t.test(
    n = NULL,
    delta = cohens_d_encoding_strategy,
    sd = 1,
    sig.level = 0.05,
    power = 0.90,
    type = "two.sample")

# Print result
cat(sprintf("Required n per group: %.6f\n", power_res$n))
```
  
  
  
# Part 3: Permutation tests  

Let’s try testing out a different question – does strategy type impact the rate of false memories? It may make more sense to use a permutation test here because false alarm rate is not normally distributed.  

Code a **permutation** test to compare `false_recall_proportion` for shallow versus deep strategy participants using the following parameters. As part of this, you should write a permutation function (called permtest <- function(samp1, samp2)) that computes a single permutation. It takes as input a dataframe, runs a single permutation, and outputs a dataframe with two columns (“mean_falsealarms_shallow” and “mean_falsealarms_deep”) and one row. 

  
## Question 8:  
a: Your permutations will be randomly shuffling / swapping condition membership (deep / shallow strategy usage).
```{r}
# Define the permutation test function
# Sources:
#   1) https://www.geeksforgeeks.org/r-language/sample-function/
#   2) https://www.geeksforgeeks.org/r-language/how-to-create-dataframe-in-r/

# Define the permutation test function (dataframe approach)
permtest <- function(df) {
  
  # Shuffle encoding depth labels
  df$encoding_depth_shuffled <- sample(df$encoding_depth)
  
  # Compute mean false recall for shuffled shallow group
  shuffled_mean_shallow <- mean(
    df$false_recall_proportion[df$encoding_depth_shuffled == "shallow"]
  )
  
  # Compute mean false recall for shuffled deep group
  shuffled_mean_deep <- mean(
    df$false_recall_proportion[df$encoding_depth_shuffled == "deep"]
  )
  
  # Create data frame
  dataframe <- data.frame(
    mean_falsealarms_shallow = shuffled_mean_shallow,
    mean_falsealarms_deep = shuffled_mean_deep
  )
  
  # Return data frame
  return(dataframe)
}
```

  
b: Use the permutation function to conduct 1000 permutations. Set the seed to 2026 before you start your permutations. Save the results in a dataframe called “**permutations**” with 1000 rows, and 2 columns called “**mean_falsealarms_shallow**” and “**mean_falsealarms_deep**”. Output the top 10 rows of the “**permutations**” dataframe.  
```{r}
# Sources:
#   1) https://www.geeksforgeeks.org/r-language/for-loop-in-r/
#   2) https://www.geeksforgeeks.org/r-language/how-to-create-dataframe-in-r/

# Set the seed to 2026
set.seed(2026)

# Create dataframe for permutation test
perm_df <- participant_level_data[
  participant_level_data$encoding_depth %in% c("shallow", "deep"),
  c("false_recall_proportion", "encoding_depth")
]

# Initialize data frame
permutations <- data.frame(
  mean_falsealarms_shallow = c(),
  mean_falsealarms_deep = c()
)

# Run permutations
for (i in 1:1000) {
  permutations <- bind_rows(permutations, permtest(perm_df))
}

# Extract first 10 rows
first_10_rows <- permutations[1:10, ]

# Log the results
knitr::kable(first_10_rows, caption = "First 10 Rows of Permuted Data")
```

  
c: Plot a **histogram** of the 1000 mean differences between the shuffled groups, and
show where the sample mean lies with a vertical line.  
```{r}
# Sources
#   1) https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/abline

# Get false recall data for permutations
shallow_false_recall_permutation_data <- permutations$mean_falsealarms_shallow
deep_false_recall_permutation_data <- permutations$mean_falsealarms_deep

# Get the differences of means vector
permutation_differences <- deep_false_recall_permutation_data - shallow_false_recall_permutation_data

# Make the histogram
hist(permutation_differences,
     main = "Permutation Test Difference of Means\nFor False Recall Proportions (n = 1000)", 
     xlab = "Difference of Means (Deep - Shallow)", 
     ylab = "Count",
     col  = "lightblue",
     )

# Get deep participants' false recall data
deep_false_recall_data <-
  participant_level_data$false_recall_proportion[
    participant_level_data$encoding_depth == "deep"]

# Get shallow participants' false recall data
shallow_false_recall_data <-
  participant_level_data$false_recall_proportion[
    participant_level_data$encoding_depth == "shallow"]

# Get sample mean and plot horizontal line
abline(v = mean(deep_false_recall_data) - mean(shallow_false_recall_data), col = "red", lwd = 2)

```

  
d: Calculate the p-value of the two-tailed permutation test. Save this p-value in a variable called “**p_val_permutation**” and output this value.  
```{r}

# Get difference of sample means
sample_diff_of_means <- mean(deep_false_recall_data) - mean(shallow_false_recall_data)

# Get number of samples whose absolute value is greater than sample difference
bool_list <- abs(permutation_differences) >= abs(sample_diff_of_means)
num_abs_greater <- 0
for (boolean in bool_list) {
  if (boolean == TRUE) {
    num_abs_greater <- num_abs_greater + 1
  }
}

# Get p value
p_val_permutation <- num_abs_greater / length(permutation_differences)

# Print result
cat(sprintf("False Recall Proportion Difference of Means Permutation P-Value: %.6f\n", p_val_permutation))
```

  
e:  Write up the permutation results in the form of a sentence for a research paper.  

A two-tailed permutation test with 1,000 permutations was conducted to examine whether false recall proportions differed between shallow and deep encoding strategies. The permutation test did not reveal a statistically significant difference between groups (p = 0.731).
 

# Part 4: Bootstrapping confidence intervals  
  
## Question 9:  
Calculate the **confidence interval** around the false recall proportion for deep versus shallow strategy individuals, using the formula mentioned in class. Place the confidence intervals into a dataframe called `Cis` with one row and the columns “ci_lower_shallow”, “ci_upper_shallow”, “ci_lower_deep”, “ci_upper_deep”.

```{r}

# Get false recall data for shallow and deep
shallow_false_recall_data <-
  participant_level_data$false_recall_proportion[
    participant_level_data$encoding_depth == "shallow"]

deep_false_recall_data <-
  participant_level_data$false_recall_proportion[
    participant_level_data$encoding_depth == "deep"]

# Sample sizes of data
n_shallow <- length(shallow_false_recall_data)
n_deep <- length(deep_false_recall_data)

# Means of each
mean_shallow <- mean(shallow_false_recall_data)
mean_deep <- mean(deep_false_recall_data)

# Standard deviations of each
sd_shallow <- sd(shallow_false_recall_data)
sd_deep <- sd(deep_false_recall_data)

# Standard errors for each
se_shallow <- sd_shallow / sqrt(n_shallow)
se_deep <- sd_deep / sqrt(n_deep)

# Critical t value for 95% CI
t_crit_shallow <- qt(0.975, df = n_shallow - 1)
t_crit_deep <- qt(0.975, df = n_deep - 1)

# Confidence intervals
ci_lower_shallow <- mean_shallow - t_crit_shallow * se_shallow
ci_upper_shallow <- mean_shallow + t_crit_shallow * se_shallow

ci_lower_deep <- mean_deep - t_crit_deep * se_deep
ci_upper_deep <- mean_deep + t_crit_deep * se_deep

# Combine into Cis dataframe
Cis <- data.frame(
  ci_lower_shallow = ci_lower_shallow,
  ci_upper_shallow = ci_upper_shallow,
  ci_lower_deep = ci_lower_deep,
  ci_upper_deep = ci_upper_deep
)

# Log the results
knitr::kable(Cis, caption = "False Recall Proportion Confidence Intervals")

```


## Question 10:  
We will now calculate confidence intervals using **bootstrapping** – this method is particularly useful for calculating statistics that don’t have an easy confidence interval formula! (Hint: The R Demo on Permutations will be helpful here when coding.)
  
a: Take a bootstrap sample from the data, of the same sample size, for "false recall proportion" for shallow strategy and deep strategy individuals separately. Calculate the sample means.   
```{r}
# Sources:
#   1) https://www.statology.org/bootstrap-confidence-intervals/
#   2) https://www.geeksforgeeks.org/r-language/sample-function/

# Take one bootstrap sample for shallow
bootstrap_shallow <- sample(shallow_false_recall_data,
                            size = length(shallow_false_recall_data),
                            replace = TRUE)

# Take one bootstrap sample for deep
bootstrap_deep <- sample(deep_false_recall_data,
                         size = length(deep_false_recall_data),
                         replace = TRUE)

# Calculate sample means
mean_bootstrap_shallow <- mean(bootstrap_shallow)
mean_bootstrap_deep <- mean(bootstrap_deep)

# Print results
cat(sprintf("Shallow Strategy Bootstrap Mean: %.6f\n", mean_bootstrap_shallow))
cat(sprintf("Deep Strategy Bootstrap Mean: %.6f\n", mean_bootstrap_deep))
```

  
  
b: Repeat 10.a for 1,000 iterations. Set the seed to 2026 before you start your permutations. Create a dataframe with your bootstraps called “**bootstraps**” and the columns “mean_falsealarms_shallow” and “mean_falsealarms_deep”. Output the first 10 rows of the “**bootstraps**” dataframe.  
```{r}
# Set seed
set.seed(2026)

# Initialize bootstrap data frame
bootstraps <- data.frame(
  mean_falsealarms_shallow = c(),
  mean_falsealarms_deep = c()
)

# Run 1000 bootstrap samples
for (i in 1:1000) {
  
  # Bootstrap samples
  bootstrap_shallow <- sample(shallow_false_recall_data,
                              size = length(shallow_false_recall_data),
                              replace = TRUE)
  
  bootstrap_deep <- sample(deep_false_recall_data,
                           size = length(deep_false_recall_data),
                           replace = TRUE)
  
  # Store means
  bootstraps <- bind_rows(
    bootstraps,
    data.frame(
      mean_falsealarms_shallow = mean(bootstrap_shallow),
      mean_falsealarms_deep = mean(bootstrap_deep)
    )
  )
}

# Output first 10 rows
knitr::kable(head(bootstraps, 10), caption = "First 10 Bootstrap Samples")
```

  
c: Plot two overlaid histograms of the bootstrap sample means for shallow strategy participants, and the bootstrap sample means for deep strategy participants.   
```{r}

# Plot shallow bootstrap means
hist(bootstraps$mean_falsealarms_shallow,
     col  = "orange",
     main = "Bootstrap Sample Means for False Recall Proportion",
     xlab = "Bootstrap Mean",
     ylab = "Count")

# Overlay deep bootstrap means
hist(bootstraps$mean_falsealarms_deep,
     col = "purple",
     add = TRUE)

# Make clear which is which
legend("topright",
       legend = c("Shallow", "Deep"),
       fill = c("orange", "purple"))

```
  
  
d: Calculate the bootstrapped 95% confidence intervals for shallow and deep strategy participants. (Hint: These are the values at the bottom and top 2.5% of your distributions.) Place the confidence intervals into a dataframe called “**Cis_boot**” with one row and the columns “ci_lower_shallow”, “ci_upper_shallow”, “ci_lower_deep”, “ci_upper_deep”. Output the “**Cis_boot**” dataframe.  
```{r}
# Sources:
#   1) https://www.statology.org/bootstrap-confidence-intervals/

# Sort bootstrap distributions
sorted_shallow <- sort(bootstraps$mean_falsealarms_shallow)
sorted_deep <- sort(bootstraps$mean_falsealarms_deep)

# Get indices for 2.5% and 97.5%
lower_index <- ceiling(0.025 * length(sorted_shallow))
upper_index <- floor(0.975 * length(sorted_shallow))

# Confidence intervals
ci_lower_shallow <- sorted_shallow[lower_index]
ci_upper_shallow <- sorted_shallow[upper_index]

ci_lower_deep <- sorted_deep[lower_index]
ci_upper_deep <- sorted_deep[upper_index]

# Combine into dataframe
Cis_boot <- data.frame(
  ci_lower_shallow = ci_lower_shallow,
  ci_upper_shallow = ci_upper_shallow,
  ci_lower_deep = ci_lower_deep,
  ci_upper_deep = ci_upper_deep
)

# Output results
knitr::kable(Cis_boot, caption = "Bootstrapped 95% Confidence Intervals")
```

  
## Question 11:  
Output the data frames with your originally calculated confidence intervals (“**Cis**” from Q9) and your bootstrapped confidence intervals (“**Cis_boot**” from Q10).  
```{r}
# Output parametric confidence intervals
knitr::kable(Cis, caption = "Parametric Confidence Intervals")

# Output bootstrapped confidence intervals
knitr::kable(Cis_boot, caption = "Bootstrapped Confidence Intervals")
```

How do the originally (parametric) calculated and bootstrapped confidence intervals compare?  

The parametric and bootstrapped confidence intervals seem to have very similar ranges, but the bootstrapped confidence intervals are narrower for both shallow and deep. I don't know exactly how signifcant this is, but I would guess that the results are consistent across the two methods.
  
What does your bootstrapped confidence interval imply about the difference (or similarity) in false memories based on depth of encoding strategy?  

The bootstrapped confidence intervals show significant overlap for shallow and deep conditions. They have nearly identical lower bounds but the shallow condition has an upper bound 0.1 points higher than the deep strategy. This could be due to chance, or could alternatively imply that the shallow strategy leads to higher variability towards the top of the range.
  

