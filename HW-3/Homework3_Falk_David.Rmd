
---
title: "Homework 3"
author: "David Falk"
date: "`r Sys.time()`"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---
<!---
In order to check that all the necessary packages are installed, Click the File > New File > R Markdown button and keep all of the options as is. Save the document somewhere, and, while that new document is open, click the knit button. If a window with some plots shows up, everything is installed correctly. If RStudio prompts you to install any packages, you should install them.
-->

<!---
Please save this file as Homework3_lastname_firstname.Rmd

Be sure to update the author field in the block above.

While you are working on your homework, you can use the green arrows to run a "chunk" of code. In order to produce the final document, use the knit button. You will turn in both the Rmd file and the knitted html file, which will appear next to wherever you saved the Rmd file if there are no errors.

Beware, if you run chunks out of order or reuse variable names your code may produce different results when it is run again from top to bottom. Before you knit your document, it is a good idea to run all the chunks in order from top to bottom in a clean environment. Use the broom button in the environment pane to clear your variables and click the Session > Restart R and Run All Chunks.

If you ever want to reference the documentation about a function, go to the console below and type ?function_name.
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
# Can set your local path to control exactly where this saves. Otherwise, will save in directory with the Rmd file
# knitr::opts_knit$set(root.dir = "/home/expdes/outputs/")  

library(tidyverse)  # hub of many packages
set.seed(1)  # reproducible results
theme_set(theme_minimal())  # nice-looking plots

```

# Part 0: Importing Data  
Import the class data from Canvas.  
```{r}

# Loading the CSVs
class_data <- read.csv(file.path("data", "class_data.csv"))
participant_level_data <- read.csv(file.path("data", "participant_level_data.csv"))

# Pre extract relevant columns
vviq_scores        <- participant_level_data$VVIQ_score
trueRecall_scores  <- participant_level_data$true_recall_proportion
falseRecall_scores <- participant_level_data$false_recall_proportion
```


   
# Part 1: Simple Linear Regression  

One question we have is whether imagery ability predicts memory performance (both true recall and false recall). will use a Simple Linear Regression to answer these questions, and we will start off using the `participant_level_data` first.  

## Question 1: Create two scatterplots: (4 points total)  

a. Plot VVIQ_score (x-axis) by true recall proportion (y-axis) for each participant. Be sure to title and label your plot appropriately and to use jitter if necessary.

```{r}

# Plot vviq by true recall
plot(
     vviq_scores,
     jitter(trueRecall_scores),
     col = "darkgreen",
     pch = 16,
     xlab = "VVIQ Score",
     ylab = "True Recall Proportion",
     xlim = c(0, max(vviq_scores)),
     ylim = c(0, 1),
     main = "True Recall Proportion Against VVIQ Score"
)
```

b. Plot VVIQ_score (x-axis) by false recall proportion (y-axis) for each participant. Be sure to title and label your plot appropriately and to use jitter if necessary.

```{r}
# Plot vviq by false recall
plot(
     vviq_scores,
     jitter(falseRecall_scores),
     col = "maroon",
     pch = 16,
     xlab = "VVIQ Score",
     ylab = "False Recall Proportion",
     xlim = c(0, max(vviq_scores)),
     ylim = c(0, 1),
     main = "False Recall Proportion Against VVIQ Score"
)
```
  
## Question 2:  
Calculate the slope (called m) and intercept (called b) of the best fit line for each scatterplot from scratch.

For each scatterplot, calculate and save the slopes (m)  and intercepts (b) into two dataframes as specified below. Output these dataframes.

Create “coefs_truerecall” with one row and two columns. The columns should be called “m_truerecall” and “b_truerecall” and should contain the slope and intercept values (respectively) for “true_recall_proportion”.
```{r}
bestfit_line = function(x_col, y_col, m_name, b_name) {
  
  # Get means
  mean_x <- mean(x_col)
  mean_y <- mean(y_col)
  
  # Get difference from means
  x_diff <- x_col - mean_x
  y_diff <- y_col - mean_y
  
  # calculate m and b
  m <- sum(x_diff * y_diff) / sum(x_diff ^ 2)
  b <- mean_y - (m * mean_x)
  
  bestfit <- data.frame(m, b)
  names(bestfit) <- c(m_name, b_name)
  return(bestfit)
}

coefs_truerecall <- bestfit_line(vviq_scores, trueRecall_scores, 
                                 "m_truerecall", "b_truerecall")
# Print the data
knitr::kable(
  coefs_truerecall,
  caption = "Line of Best Fit Data (True Recall)"
)
```
  
Create “coefs_falserecall” with one row and two columns. The columns should be called “m_falserecall” and “b_falserecall” and should contain the slope and intercept values (respectively) for “false_recall_proportion”.
```{r}
coefs_falserecall <- bestfit_line(vviq_scores, falseRecall_scores, 
                                 "m_falserecall", "b_falserecall")
# Print the data
knitr::kable(
  coefs_falserecall,
  caption = "Line of Best Fit Data (False Recall)"
)
```


### 2.a:  
Using your slope and intercept, plot the best fit line on top of the previous plot. Be sure to extend your plot boundaries to show the intercept.
```{r}
# Plot vviq by true recall
plot(
     vviq_scores,
     jitter(trueRecall_scores),
     col = "darkgreen",
     pch = 16,
     xlab = "VVIQ Score",
     ylab = "True Recall Proportion",
     xlim = c(0, max(vviq_scores)),
     ylim = c(min(0, coefs_truerecall$b_truerecall), 1),
     main = "True Recall Proportion Against VVIQ Score"
)
abline(a = coefs_truerecall$b_truerecall[1],
       b = coefs_truerecall$m_truerecall[1],
       col = "orange",
       lwd = 3)
```
  
```{r}
plot(
     vviq_scores,
     jitter(falseRecall_scores),
     col = "maroon",
     pch = 16,
     xlab = "VVIQ Score",
     ylab = "False Recall Proportion",
     xlim = c(0, max(vviq_scores)),
     ylim = c(min(0, coefs_falserecall$b_falserecall), 1),
     main = "False Recall Proportion Against VVIQ Score"
)
abline(a = coefs_falserecall$b_falserecall[1],
       b = coefs_falserecall$m_falserecall[1],
       col = "orange",
       lwd = 3)
```


### 2.b:  
For each best fit line, write the linear function in slope-intercept form.
True Recall:  y =  0.0002886189 * x + 0.5168375
False Recall: y = -0.0036113260 * x + 0.5483042

  
### 2.c:  
For each scatterplot, briefly discuss: Visually looking at this best fit line, what can you describe about the slope and the relationship between these two variables? How does y change as you increase 1 unit in x?

For true recall proportion there is a nearly flat (ever so slightly positive slope) relationship between VVIQ score and true recall proportion. This implies that modulating VVIQ score has very little effect on true recall proportion. As you increase one unit along the x axis (vviq score) you see a change of 0.0002886189 along the y axis (true recall proportion).  

For false recall proportion there is a negative relationship between VVIQ score and false recall proportion As you increase one unit along the x axis (vviq score) you see a change of -0.0036113260 along the y axis (false recall proportion).  

  
## Question 3:  
For each comparison, calculate the residuals.

Create a new dataframe from `participant_level_data` called “truerecall_resid_bestfit” that contains the residuals of the model using “VVIQ_score” to predict “true_recall_proportion”. Output the first ten rows of the dataframe.   
```{r}
calculate_residuals = function(x_col, y_col, coefs) {
  
  # Extract slope and intercept from coefficients
  m <- coefs[1, 1]
  b <- coefs[1, 2]
  
  # Calculate predicted values for x column
  pred_vals <- (m * x_col) + b
  
  # Get residuals
  residuals <- data.frame(residuals = y_col - pred_vals)
  
  return(residuals)
}

truerecall_resid_bestfit <- calculate_residuals(vviq_scores, trueRecall_scores, coefs_truerecall)

# Print the data
knitr::kable(
  head(truerecall_resid_bestfit, 10),
  caption = "True Recall Residuals (First 10 Rows)"
)
```
  
Create a new dataframe from `participant_level_data` called “falserecall_resid_bestfit” that contains the residuals of the model using “VVIQ_score” to predict “false_recall_proportion”. Output the first ten lines of the dataframe. 
```{r}
falserecall_resid_bestfit <- calculate_residuals(vviq_scores, falseRecall_scores, coefs_falserecall)

# Print the data
knitr::kable(
  head(falserecall_resid_bestfit, 10),
  caption = "False Recall Residuals (First 10 Rows)"
)
```

  
For each best fit line, create a residuals plot.
```{r}
overall_min_resid <- min(min(truerecall_resid_bestfit$residuals), min(falserecall_resid_bestfit$residuals))
overall_max_resid <- max(max(truerecall_resid_bestfit$residuals), max(falserecall_resid_bestfit$residuals))
plot(
     vviq_scores,
     jitter(truerecall_resid_bestfit$residuals),
     col = "orange",
     pch = 16,
     xlab = "VVIQ Score",
     ylab = "Residual",
     xlim = c(0, max(vviq_scores)),
     ylim = c(overall_min_resid, overall_max_resid),
     main = "True Recall Residuals Against VVIQ Score"
)
```
  
```{r}
plot(
     vviq_scores,
     jitter(falserecall_resid_bestfit$residuals),
     col = "maroon",
     pch = 16,
     xlab = "VVIQ Score",
     ylab = "Residual",
     xlim = c(min(vviq_scores), max(vviq_scores)),
     ylim = c(overall_min_resid, overall_max_resid),
     main = "False Recall Residuals Against VVIQ Score"
)
```

  
For each residual plot, briefly discuss: Do you see any patterns here? What does this mean about the linear fit, based on what you do/don’t see in the residuals? 

True recall residuals:
The residuals seem to be randomly scattered around zero with roughly constant spread across VVIQ scores. There isn't a clear pattern or curvature. This suggests that the linear model is appropriate and that assumptions about linearity and homoskedasticity are reasonably satisfied.

False recall residuals:
The residuals are more variable and may there might be an effect of VVIQ score on residual, particularly at higher values. This could mean mild heteroskedasticity or a potential nonlinear relationship. However, there is no strong systematic pattern, so I think that the linear model is still reasonable, though the fit appears weaker than for true recall.



## Question 4:
Calculate the best fit lines using a built-in R function. 
```{r}
# Best fit line for VVIQ & true recall proportion

# Get initial values (apparently not a dataframe)
builtin_true  <- lm(trueRecall_scores ~ vviq_scores)

# Convert data to dataframes
builtin_coefs_truerecall  <- data.frame(
                        m_truerecall = builtin_true$coefficients[2],
                        b_truerecall = builtin_true$coefficients[1])

# Log the results in line and equation form
knitr::kable(
  builtin_coefs_truerecall,
  caption = "Line of Best Fit Data (True Recall)"
)
cat(sprintf("True Recall Line of Best Fit: y = %.6f * x + %.6f", builtin_coefs_truerecall$m_truerecall, builtin_coefs_truerecall$b_truerecall)
)
```
  
```{r}
# Best fit line for VVIQ & false recall proportion

# Get initial values (apparently not a dataframe)
builtin_false <- lm(falseRecall_scores ~ vviq_scores)

# Convert data to dataframes
builtin_coefs_falserecall <- data.frame(
                        m_falserecall = builtin_false$coefficients[2],
                        b_falserecall = builtin_false$coefficients[1])

# Log the results in table and equation form
knitr::kable(
  builtin_coefs_falserecall,
  caption = "Line of Best Fit Data (False Recall)"
)
cat(sprintf("False Recall Line of Best Fit: y = %.6f * x + %.6f", builtin_coefs_falserecall$m_falserecall, builtin_coefs_falserecall$b_falserecall)
)
```

How do these lines compare to the ones you calculated from scratch?
The values for my function and the built-in function are identical.


  

# Part 2: Multiple Regression    
  
## Question 5:  
Now we can finally put all the pieces together from the experiment to answer one of our main questions! Essentially: how do encoding strategy, interference type, and imagery ability impact memory performance? Let’s try testing this out with a multiple regression! We will now turn to using the `class_data` (because we want to use some within subjects factors here).    

Standardize all variables before including them in your regression. Add the standardized variables to “class_data” and call them “true_recall_proportion_z” and “vviq_z”. Output the first 10 rows of “class_data”.
```{r}


# Add values to class data
class_data$true_recall_proportion_z <- scale(class_data$true_recall_proportion)
class_data$vviq_z <- scale(class_data$VVIQ_score)

# Log the results
knitr::kable(
  head(class_data, 10),
  caption = "Class Data (First 10 Rows)"
)
```

Using R built-in functions, calculate the multiple linear regression fit for a model with the following variables. Call this model "fit".  Output the summary of the model.  

Variables:  
a. Encoding strategy as a categorical IV (as 4 categories).  
b. Interference type as a categorical IV.  
c. VVIQ score as a continuous IV.  
d. True recall performance as a continuous DV.    
```{r}

# Let's do this up front so I don't have to do this every time
class_data$encoding_strategy <- as.factor(class_data$encoding_strategy)
class_data$interference_type <- as.factor(class_data$interference_type)

# Calculate the multiple linear regression
fit <- lm(
          true_recall_proportion_z ~
          encoding_strategy + interference_type + vviq_z,
          data = class_data
)

# Print the summary
summary(fit)
```


  
## Question 6:  
Report the best fit plane function from Question 5.

Let's define some variables first...

True Recall Proportion (z-scored):
  Denote this as TRP_z
  
Encoding Strategy:
  Reference Category: imag_association
  Let ES_1 = 1 if imag_single, 0 otherwise      (coefficient = "B_1")
  Let ES_2 = 1 if verb_association, 0 otherwise (coefficient = "B_2")
  Let ES_3 = 1 if verb_single, 0 otherwise      (coefficient = "B_3")

Interference Type:
  Reference Category : control
  Let IT_1 = 1 if verbal, 0 otherwise           (coefficient = "B_4")
  Let IT_2 = 1 if visual, 0 otherwise           (coefficient = "B_5")

VVIQ Score (z-scored):
  Denote this as VVIQ_z                         (coefficient = "B_6")
  
Now our plane equation is...

TRP_z = Intercept + (B_1 * ES_1) + (B_2 * ES_2) + (B_3 * ES_3) + (B_4 * IT_1) + (B_5 * IT_2) + (B_6 * VVIQ_z)

and plugging in values for the B_i's we get...

TRP_z = 0.35264 + (-0.39064 * ES_1) + (-0.10819 * ES_2) + (-0.19955 * ES_3) + (-0.30219 * IT_1) + (-0.24823 * IT_2) + (0.05359 * VVIQ_z)
  
## Question 7:  
Write code that partitions the variance by doing the following steps (parts a-c):

*HINT - Continue to use the standardized variables when computing the SSE and onwards*

### 7.a:  
Calculate the sum of square errors (SSE) from scratch. Save this in a variable called “SSE” and output the value.
```{r}

# Get predicted values
pred_trp <- predict(fit)

# Get squared differences of errors
sqrd_errs <- (class_data$true_recall_proportion_z - pred_trp) ^ 2

# Get SSE
SSE <- sum(sqrd_errs)

cat(sprintf("Sum of Square Errors (SSE): %.6f", SSE))
```

  
### 7.b:  
Calculate the sum of square regression (SSR) from scratch. Save this in a variable called “SSR” and output the value. 
```{r}
# Actually we shouldn't need this block since we z scored so mean is just 0
# ==========================================================================
# Get mean of predicted values
# mean_trp <- mean(class_data$true_recall_proportion_z)

# Get squared residuals
# sqrd_diffs <- (pred_trp - mean_trp) ^ 2
# ==========================================================================

# Get SSR
SSR <- sum(pred_trp ^ 2)

cat(sprintf("Sum of Square Regression (SSR): %.6f", SSR))
```

  
  
### 7.c:  
Calculate the sum of square total (SST) from scratch. Save this in a variable called “SST” and output the value.
```{r}
# Similarly we can just sum and square because we centered DV around 0
# ====================================================================

SST <- sum(class_data$true_recall_proportion_z ^ 2)
cat(sprintf("Sum of Square Total (SST): %.6f", SST))
```
  
  
## Question 8:  
Briefly describe: What do these variance components tell us about the amount of variance explained by the regression?

The total variance in true recall (SST = 77) is partitioned into explained variance (SSR = 3.066) and unexplained variance (SSE = 73.934). The variance explained is very small relative to the total variance, which means that the predictors account for only a small fraction of the variability in true recall. Most of the variance is in the unexplained variance term (SSE). This suggests that the regression model has relatively weak explanatory power with respect to the DV, true recall performance.



## Question 9:  
Write code that further partitions the variance using a reduced model. Show these calculations from scratch.

### 9.a:  
How much of the SSR is unique to encoding strategy? Call this variable “UniqueSSR_encoding_strategy” and output the value.
```{r}

# Calculate the multiple linear regression without including the encoding strategy
fit_no_encoding <- lm(
          true_recall_proportion_z ~
          interference_type + vviq_z,
          data = class_data
)

# Get predicted true recall performance for no encoding model
pred_trp_no_encoding <- predict(fit_no_encoding)

# Get SSR for no encoding
SSR_no_encoding <- sum(pred_trp_no_encoding ^ 2)

# Subtract off no encoding SSR
UniqueSSR_encoding_strategy <- SSR - SSR_no_encoding

# Print results
cat(sprintf("SSR unique to encoding strategy: %.6f", UniqueSSR_encoding_strategy))
```

  
### 9.b:  
How much of the SSR is unique to VVIQ score? Call this variable “UniqueSSR_vviq” and output the value. 
```{r}

# Calculate the multiple linear regression without including vviq
fit_no_vviq <- lm(
          true_recall_proportion_z ~
          encoding_strategy + interference_type,
          data = class_data
)

# Get predicted true recall performance for no vviq
pred_trp_no_vviq <- predict(fit_no_vviq)

# Get SSR for no vviq
SSR_no_vviq <- sum(predict(fit_no_vviq) ^ 2)

# Subtract off no vviq SSR
UniqueSSR_vviq <- SSR - SSR_no_vviq

# Print results
cat(sprintf("SSR unique to VVIQ: %.6f", UniqueSSR_vviq))
```
  
### 9.c:  
How much of the SSR is shared between these two factors? Call this variable “Shared_SSR” and output the value. 
```{r}
# Working under the assumption that we want ONLY SSR shared by encoding and vviq...

# Calculate interference only SSR
fit_interference_only <- lm(
  true_recall_proportion_z ~ interference_type,
  data = class_data
)
SSR_interference_only <- sum(predict(fit_interference_only) ^ 2)

# Thinking in terms of ...
# Total_SSR =
#   All SSR in interference (including all overlaps with encoding, VVIQ, and encoding + VVIQ)
# + SSR unique to encoding 
# + encoding unique to VVIQ
Shared_SSR <- (SSR - SSR_interference_only) -
              UniqueSSR_encoding_strategy -
              UniqueSSR_vviq


# Print results
cat(sprintf("SSR shared by ONLY encoding strategy and VVIQ: %.6f", Shared_SSR))
```
  
  
  
## Question 10: 

### 10.a:  
Calculate R^2 goodness of fit **from scratch**. Save this in a variable called “Rsq” and output the value.
```{r}

# Compute R^2
Rsq <- SSR / SST

# Print results
cat(sprintf("Full Model R^2: %.6f", Rsq))
```

  
### 10.b:  
Calculate Adjusted R^2 **from scratch**. Save this in a variable called “Adj_Rsq” and output the value. 
```{r}

# Number of data points
n <- nrow(class_data)

# Number of predictors
k <- 6

# Get adjusted R^2
numerator <- SSE / (n - k - 1)
denominator <- SST / (n - 1)
Adj_Rsq <- 1 - (numerator / denominator)

# Print results
cat(sprintf("Adjusted R^2: %.6f", Adj_Rsq))
```

Briefly discuss how the relationship between R^2 and Adjusted R^2 here relates to the ideal number of predictors in the model. 

Adjusted R^2 penalizes models that use more predictors because they could just be overfitting. In this case R^2 is small (0.04) and adjusted R^2 is negative (-0.04). This means that after we penalize for the number of predictors, this model performs worse than if we just estimated true recall proportion from the intercept alone.
  
### 10.c:  
Calculate Cohen’s f2 for effect size **from scratch**. Save this in a variable called “cohen_fsq” and output the value.
```{r}

# Calculate it
cohen_fsq <- Rsq / (1 - Rsq)

# Print it
cat(sprintf("Cohen's f^2: %.6f", cohen_fsq))
```
  
  
### 10.d:  
Calculate the F-statistic and corresponding p-value for overall model significance **from scratch**. Save these in separate variables called “F_stat” and “pval_F” and output these values.
```{r}

# Get the degrees of freedom for predictors and residuals
predictors_df <- k
residuals_df <- n - k - 1

# Compute F statistic
numerator <- SSR / predictors_df
denominator <- SSE / residuals_df
F_stat <- numerator / denominator

# Compute p-value
pval_F <- 1 - pf(F_stat, df1 = predictors_df, df2 = residuals_df)

cat(sprintf("F-statistic: %.6f\n", F_stat))
cat(sprintf("p-value: %.6f", pval_F))
```


  
## Question 11:  
Calculate these metrics using built-in R functions and and report those here. 
```{r}

# Just goin' right on ahead and usin' the built in functions
# ==========================================================

# R^2
builtin_Rsq <- summary(fit)$r.squared

# Adjusted R^2
builtin_Adj_Rsq <- summary(fit)$adj.r.squared

# F statistic
builtin_F_stat <- summary(fit)$fstatistic[1]

# F statistic p val
builtin_pval_F <- pf(summary(fit)$fstatistic[1],
                    summary(fit)$fstatistic[2],
                    summary(fit)$fstatistic[3],
                    lower.tail = FALSE)

# Print the results
cat(sprintf("Built In Functions Statistics\n"))
cat(sprintf("=============================\n"))
cat(sprintf("Built-in R^2: %.6f\n", builtin_Rsq))
cat(sprintf("Built-in Adjusted R^2: %.6f\n", builtin_Adj_Rsq))
cat(sprintf("Built-in F-statistic: %.6f\n", builtin_F_stat))
cat(sprintf("Built-in p-value: %.6f", builtin_pval_F))
```


Additionally, calculate t-statistics and corresponding p-values for each predictor using built-in R functions.  
```{r}

# Get the coefficients for each predictor
coefficients_table <- summary(fit)$coefficients

# Extract the column with the t statistics
t_stat <- coefficients_table[, "t value"]

# Extract the column with the p vals
pval_t <- coefficients_table[, "Pr(>|t|)"]

# Turn into a dataframe for nice printing :)
t_p_table  <- data.frame(
  predictor = rownames(coefficients_table),
  t_Statistic = t_stat,
  p_Value = pval_t,
  row.names = NULL
)

# Print the results
knitr::kable(
    t_p_table, 
    caption = "t-statistics and p-values for each predictor")
```


## Question 12:  
Report all of the calculated information from the multiple regression as if reporting in a research paper.

We computed a multiple linear regression to examine the extent to which encoding strategy, interference type, and VVIQ score (z scored) predicted true recall performance (z scored).

The overall model was not statistically significant (F(6, 71) = 0.491, p = 0.813) and explained roughly 4% of the observed variance in true recall performance (R^2 = 0.0398, Adjusted R^2 = -0.0413).

Furthermore, none of the individual predictors were statistically significant. Relative to the reference categories (imagery association for encoding strategy and control for interference type), no encoding condition achieved statistical significance (all p > 0.23, 3 predictors) and no interference type achieved statistical significance (all p > 0.28, 2 predictors). VVIQ (z scored) was not a significant predictor of recall either (t(71) = 0.392, p = 0.697).

Effect size for the model was small (f^2 = 0.042) which suggests a small effect size and minimal explanatory power. Overall, the predictors included in the model did not meaningfully explain variation in true recall performance. 


  
## Question 13:  
Re-run the model with a random effect of participant added. (Now you have a mixed effects model!). You can use built-in functions for this. Run your "model_updated".   

```{r}
library(lme4)
library(lmerTest)

# Calculate the mixed effects model 
# Currently allows intercept to vary by participant (different baselines)
# But keeps slopes for each participant fixed
model_updated <- lmer(
  true_recall_proportion_z ~ 
    encoding_strategy + interference_type + vviq_z +
    (1 | participant),
  data = class_data
)

# Display the summary stats
summary(model_updated)
```


### 13.a:  
Describe why we would include a random effect of participant.   

We include a random effect of participant because each participant is responsible for multiple rows. These observations are not independent. In model_updated we are testing under the assumption that the effect of our other predictors are roughly constant across participants but that different participants may have different recall performance baselines. 

  
### 13.b:  
Report the best fit plane function of your new model.
Let's define some variables first...

True Recall Proportion (z-scored):
Denote this as TRP_z

Encoding Strategy:
Reference Category: imag_association
Let ES_1 = 1 if imag_single, 0 otherwise (coefficient = "B_1")
Let ES_2 = 1 if verb_association, 0 otherwise (coefficient = "B_2")
Let ES_3 = 1 if verb_single, 0 otherwise (coefficient = "B_3")

Interference Type:
Reference Category : control
Let IT_1 = 1 if verbal, 0 otherwise (coefficient = "B_4")
Let IT_2 = 1 if visual, 0 otherwise (coefficient = "B_5")

VVIQ Score (z-scored):
Denote this as VVIQ_z (coefficient = "B_6")

Random Effect of Participant:
Denote the random intercept for participant j as u_j
u_j ~ N(0, 0.8295)

Residual Error:
Denote this as e_ij
e_ij ~ N(0, 0.3053)

Now our plane equation is...

TRP_z_ij = Intercept + (B_1 * ES_1) + (B_2 * ES_2) + (B_3 * ES_3) + (B_4 * IT_1) + (B_5 * IT_2) + (B_6 * VVIQ_z) + u_j + e_ij

and plugging in values for the B_i's we get...

TRP_z_ij = 0.35264 + (-0.39064 * ES_1) + (-0.10819 * ES_2) + (-0.19955 * ES_3) + (-0.30219 * IT_1) + (-0.24823 * IT_2) + (0.05359 * VVIQ_z) + u_j + e_ij


### 13.c:  
Calculate (using built-in functions is fine) the beta, t-statistic, Standard Error (SE) for each predictor and their p-values.   

*Hint: Since the standard lmer function does not provide p-values by default, we recommend using the lmerTest package. Simply loading it with library(lmerTest) before running your model will automatically add p-values to your summary() output.*  

```{r}
# Extract fixed effects table from summary
coef_table <- summary(model_updated)$coefficients

# Put into a dataframe with the requested stats
mixed_table <- data.frame(
  predictor = rownames(coef_table),
  beta = coef_table[, "Estimate"],
  SE = coef_table[, "Std. Error"],
  t_statistic = coef_table[, "t value"],
  p_value = coef_table[, "Pr(>|t|)"],
  row.names = NULL
)

# Output table
knitr::kable(
  mixed_table,
  caption = "Mixed Effects Model: Betas, SEs, t-statistics, and p-values"
)

``` 
  
### 13.d:  
Report the results (computed in 13.c)  as if reporting in a research paper.

We computed a linear mixed effects model to examine whether encoding strategy, interference type, and VVIQ score (z scored) predicted true recall performance (z scored), while including a random intercept for participant.

The intercept was not statistically significant (b = 0.35264, SE = 0.41033, t(23.07) = 0.859, p = 0.3990). Relative to the reference category imag_association, none of the encoding strategy conditions significantly predicted true recall (imag_single: b = -0.39064, SE = 0.53799, t(21) = -0.726, p = 0.4758; verb_association: b = -0.10819, SE = 0.55090, t(21) = -0.196, p = 0.8462; verb_single: b = -0.19955, SE = 0.69664, t(21) = -0.286, p = 0.7773).

Relative to the control interference condition, interference_typeverbal was marginal (b = -0.30219, SE = 0.15324, t(50) = -1.972, p = 0.0542), while interference_typevisual was not significant (b = -0.24823, SE = 0.15324, t(50) = -1.620, p = 0.1115). VVIQ_z was not a significant predictor of true recall (b = 0.05359, SE = 0.22417, t(21) = 0.239, p = 0.8134).

The model included a random intercept for participant (variance = 0.8295, SD = 0.9108), with residual variance = 0.3053 (SD = 0.5525).


### 13.e:  
Briefly discuss: How did your change(s) impact the weight of each predictor?


Adding the random intercept for participant did not change the fixed effect beta weights compared to the original regression (the estimates for encoding strategy, interference type, and VVIQ_z are the same). The main impact is that the model now explicitly accounts for within-participant dependence by allowing each participant to have their own baseline recall level. In this fit, between-participant variability is substantial (random intercept variance = 0.8295) relative to the residual variance (0.3053), which suggests that a lot of the variability in recall is due to stable differences between participants rather than changes driven by the fixed predictors. 


