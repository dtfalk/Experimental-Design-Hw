---
title: "Homework 1"
author: "David Falk"
date: "`r Sys.time()`"
output: 
  html_document:
    toc: true
---

<!---
In order to check that all the necessary packages are installed, Click the File > New File > R Markdown button and keep all of the options as is. Save the document somewhere, and, while that new document is open, click the knit button. If a window with some plots shows up, everything is installed correctly. If RStudio prompts you to install any packages, you should install them.
-->

<!---
Please save this file as Homework1_lastname_firstname.Rmd

Be sure to update the author field in the block above.

While you are working on your homework, you can use the green arrows to run a "chunk" of code. In order to produce the final document, use the knit button. You will turn in both the Rmd file and the knitted html file, which will appear next to wherever you saved the Rmd file if there are no errors.

Beware, if you run chunks out of order or reuse variable names your code may produce different results when it is run again from top to bottom. Before you knit your document, it is a good idea to run all the chunks in order from top to bottom in a clean environment. Use the broom button in the environment pane to clear your variables and click the Session > Restart R and Run All Chunks.

If you ever want to reference the documentation about a function, go to the console below and type ?function_name. Try it now with ?read_csv.
-->
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
# Can set your local path to control exactly where this saves. Otherwise, will save in directory with the Rmd file
# knitr::opts_knit$set(root.dir = "/home/expdes/outputs/")  

library(tidyverse)  # loads many useful packages
library(dplyr)
library(ggplot2)

set.seed(1)  # Ensures reproducible results
theme_set(theme_minimal())  # My personal preference for less ugly default plots

```

# Part 1: Importing Data

  1. Write code to import the data into a dataframe called `class_data`. Each row should be a participant, and the columns should include the following (2 points):  
      a. **participant** – the participant ID  
      b. **encoding_strategy** – the strategy participants were told to use to visualize the list (shallow verbal, deep verbal, shallow visual, or deep visual). This is a between-subjects measure.  
      c. **interference_type** – the type of interference present during the task (verbal, visual, or control). This is a within-subjects measure.  
      d. **VVIQ_score** – the participant’s score on the Vividness of Visual Imagery Questionnaire. The score can range from 16 to 80 points. 32 points and under is considered aphantasic (no visual imagery). This is a between-subjects measure.  
      e. **true_recall_proportion** – the proportion of trials correctly recalled.  
      f. **false_recall_proportion** – the proportion of trials containing falsely recalled items (i.e., false memories).
      
Note that we will not use all of these columns in this assignment, but we will in future ones! 

These data come from three sources - so you’ll need to combine them using R for your dataframe. (Hint: look into dplyr’s `join` functions: https://dplyr.tidyverse.org/reference/mutate-joins.html). Specifically, true_recall_summary.csv has the participant, their interference type, the block’s encoding strategy, and the true recall proportion. false_recall_summary.csv has the participant, their interference type, the block’s encoding strategy, and the false recall proportion. Finally, VVIQ_summary.csv has the participant and their VVIQ score (the value under total_score).

**Display the dataframe.**
```{r 1.1}
# Sources:
  # 1) https://www.digitalocean.com/community/tutorials/r-read-csv-file-into-data-frame
  # 2) https://stackoverflow.com/questions/13110076/function-to-concatenate-paths 
  # 3) https://dplyr.tidyverse.org/reference/mutate-joins.html

# Import each CSV into a dataframe
true_recall_data <- read.csv(file.path("class-data", "true_recall_summary.csv"))
false_recall_data <- read.csv(file.path("class-data", "false_recall_summary.csv"))
vviq_data <- read.csv(file.path("class-data", "VVIQ_summary.csv"))

# Merge the three data into a single dataframe called class_data
class_data_recall <- full_join(true_recall_data, false_recall_data)
class_data <- full_join(class_data_recall, vviq_data)


```

  2. Also note that this data is currently aggregated by block type, with each participant represented multiple times (for each of the different interference types). Let’s temporarily make a dataframe that only looks at data at the level of the participant. Make a dataframe called `participant_level_data`. Each row should be a participant and include these columns, calculated as described here (2 points):  
      a. **participant** – the same as above
      b. **encoding_strategy** – the same as above
      c. **encoding_modality** – a categorical value capturing whether the encoding strategy is visual (shallow visual and deep visual) or verbal (shallow verbal and deep verbal)  
      d. **encoding_depth** – a categorical value capturing whether the encoding strategy is deep (deep visual and deep verbal) or shallow (shallow visual and shallow verbal)
      e. **VVIQ_score** – the same as above
      f. **true_recall_proportion** – average the class_data’s true_recall_proportion, within each participant.  
      g. **false_recall_proportion** – average the class_data’s false_recall_proportion, within each participant.

**Display the dataframe.**
```{r 2}
# Sources:
  # 1) https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/ifelse
  # 2) https://dplyr.tidyverse.org/reference/group_by.html
  # 3) https://stringr.tidyverse.org/articles/stringr.html

# Group all participants by participant id
grouped_class_data <- group_by(class_data, participant)

# Create the new dataframe
participant_level_data <- summarise(grouped_class_data,
                                    participant = first(participant),
                                    encoding_strategy = first(encoding_strategy),
                                    encoding_modality = ifelse(str_detect(first(encoding_strategy), "imag"), "visual", "verbal"),
                                    encoding_depth = ifelse(str_detect(first(encoding_strategy), "single"), "shallow", "deep"),
                                    VVIQ_score = first(total_score),
                                    true_recall_proportion = mean(true_recall_proportion),
                                    false_recall_proportion = mean(false_recall_proportion)
                                    )


```

# Part 2: Simulating Data

We will now write code that simulates toy data. This is helpful for testing out your code with data that you know should follow specific patterns. Note that it will look different from the raw example data. The dataframe should have the same dimensionality as the participant_level_data, with the same column names.

  3. First, generate a toy dataset called `toy_set_a` that shows a **large difference** in true recall proportion between the verbal and visual strategies. Here are the constraints:  
    1. This dataset should have the same number of rows as the original data, and half of the toy participants should be assigned verbal strategies and half should be assigned visual strategies  
    2. The difference in true recall proportion should be at least a difference of 0.25 (higher for the visual strategies)  
    3. Simulated data should include random noise, from a normal distribution with mean 0 and SD 0.1  
    4. Set the seed to 67 before generating the “verbal_” data and set the seed to 75 before generating the “visual_” data
    
**Display the dataframe.**
```{r 3}
# Sources:
  # 1) https://www.dataquest.io/blog/how-to-create-a-dataframe-in-r/
  # 2) https://www.digitalocean.com/community/tutorials/get-number-of-rows-and-columns-in-r
  # 3) https://stat.ethz.ch/R-manual/R-devel/library/base/html/Arithmetic.html
  # 4) https://www.spsanderson.com/steveondata/posts/2025-05-26/
  # 5) https://stackoverflow.com/questions/8169323/r-concatenate-two-dataframes

# Gets the number of subjects (rows) in the actual data
num_rows <- nrow(participant_level_data)

# Floor division to handle odd num participants (not true here but why not future-proof)
half_rows <- num_rows %/% 2
  
# Floor division to handle odd num participants (not true here but why not future-proof)
quarter_rows <- half_rows %/% 2

# make data for verbal condition
set.seed(67)

# verbal shallow data
verbal_shallow_a <- data.frame(
  participant             = 1:quarter_rows,
  encoding_strategy       = "verb_single",
  encoding_modality       = "verbal",
  encoding_depth          = "shallow",
  VVIQ_score              = sample(21:80, quarter_rows),
  true_recall_proportion  = rnorm(quarter_rows, mean = 0.30, sd = 0.1),
  false_recall_proportion = rnorm(quarter_rows, mean = 0.50, sd = 0.25)
)

# verbal deep data
verbal_deep_a <- data.frame(
  participant             = (quarter_rows + 1):half_rows,
  encoding_strategy       = "verb_association",
  encoding_modality       = "verbal",
  encoding_depth          = "deep",
  VVIQ_score              = sample(21:80, half_rows - quarter_rows),
  true_recall_proportion  = rnorm(half_rows - quarter_rows, mean = 0.30, sd = 0.1),
  false_recall_proportion = rnorm(half_rows - quarter_rows, mean = 0.50, sd = 0.25)
)

# Get the verbal part of the data set
verbal_data_a <- rbind(verbal_shallow_a, verbal_deep_a)

# make data for visual condition
set.seed(75)

# visual shallow data
visual_shallow_a <- data.frame(
  participant             = (half_rows + 1):(half_rows + quarter_rows),
  encoding_strategy       = "imag_single",
  encoding_modality       = "visual",
  encoding_depth          = "shallow",
  VVIQ_score              = sample(21:80, quarter_rows),
  true_recall_proportion  = rnorm(quarter_rows, mean = 0.70, sd = 0.1),
  false_recall_proportion = rnorm(quarter_rows, mean = 0.50, sd = 0.25)
)

# visual deep data
visual_deep_a <- data.frame(
  participant             = (half_rows + quarter_rows + 1):num_rows,
  encoding_strategy       = "imag_association",
  encoding_modality       = "visual",
  encoding_depth          = "deep",
  VVIQ_score              = sample(21:80, num_rows - half_rows - quarter_rows),
  true_recall_proportion  = rnorm(num_rows - half_rows - quarter_rows, mean = 0.70, sd = 0.1),
  false_recall_proportion = rnorm(num_rows - half_rows - quarter_rows, mean = 0.50, sd = 0.25)
)

# Get the visual part of the data set
visual_data_a <- rbind(visual_shallow_a, visual_deep_a)

# Get the full data set
toy_set_a <- rbind(verbal_data_a, visual_data_a)



```
    
   3a. Calculate the **mean** and **standard error** of memory sensitivity for each of the groups. You may use built-in functions (6 points).   
  
**Note:** output these values so that they are displayed in the HTML/PDF once knit. This applies for all named values that we ask you to compute.
  
```{r 3a}
# Sources:
  # 1) https://dplyr.tidyverse.org/reference/summarise.html
  # 2) https://dplyr.tidyverse.org/reference/filter.html

# Group by modality (visual vs verbal)
grouped_toy_set_a <- group_by(toy_set_a, encoding_modality)

# Get mean and se for visual and verbal group 
group_info_a <- summarise(
  grouped_toy_set_a, 
  mean = mean(true_recall_proportion), 
  se = sd(true_recall_proportion) / sqrt(n())
)

# Extract verbal values
verbal_mean_a <- group_info_a$mean[group_info_a$encoding_modality == "verbal"]
verbal_se_a   <- group_info_a$se[group_info_a$encoding_modality == "verbal"]

# Extract visual values
visual_mean_a <- group_info_a$mean[group_info_a$encoding_modality == "visual"]
visual_se_a   <- group_info_a$se[group_info_a$encoding_modality == "visual"]
```


  4. Next, simulate a separate set of toy data called `toy_set_b` of the same size as the toy_set_a, that shows **no meaningful difference** in performance between verbal and visual strategies.  
    1. Simulated data should include random noise, from a normal distribution with mean 0 and SD 0.01.  
    2. Set the seed to 62 before generating the “verbal_” data and set the seed to 92 before generating the “visual_” data. All other constraints are same as above.

**Display the dataframe.**

```{r 4}
# Sources:
  # 1) https://www.dataquest.io/blog/how-to-create-a-dataframe-in-r/
  # 2) https://www.digitalocean.com/community/tutorials/get-number-of-rows-and-columns-in-r
  # 3) https://stat.ethz.ch/R-manual/R-devel/library/base/html/Arithmetic.html
  # 4) https://www.spsanderson.com/steveondata/posts/2025-05-26/
  # 5) https://stackoverflow.com/questions/8169323/r-concatenate-two-dataframes

# Gets the number of subjects (rows) in the actual data
num_rows <- nrow(participant_level_data)

# Floor division to handle odd num participants (not true here but why not future-proof)
half_rows <- num_rows %/% 2
  
# Floor division to handle odd num participants (not true here but why not future-proof)
quarter_rows <- half_rows %/% 2

# make data for verbal condition
set.seed(62)

# verbal shallow data
verbal_shallow_b <- data.frame(
  participant             = 1:quarter_rows,
  encoding_strategy       = "verb_single",
  encoding_modality       = "verbal",
  encoding_depth          = "shallow",
  VVIQ_score              = sample(21:80, quarter_rows),
  true_recall_proportion  = rnorm(quarter_rows, mean = 0.50, sd = 0.1),
  false_recall_proportion = rnorm(quarter_rows, mean = 0.50, sd = 0.25)
)

# verbal deep data
verbal_deep_b <- data.frame(
  participant             = (quarter_rows + 1):half_rows,
  encoding_strategy       = "verb_association",
  encoding_modality       = "verbal",
  encoding_depth          = "deep",
  VVIQ_score              = sample(21:80, half_rows - quarter_rows),
  true_recall_proportion  = rnorm(half_rows - quarter_rows, mean = 0.50, sd = 0.1),
  false_recall_proportion = rnorm(half_rows - quarter_rows, mean = 0.50, sd = 0.25)
)

# Get the verbal part of the data set
verbal_data_b <- rbind(verbal_shallow_b, verbal_deep_b)

# make data for visual condition
set.seed(92)

# visual shallow data
visual_shallow_b <- data.frame(
  participant             = (half_rows + 1):(half_rows + quarter_rows),
  encoding_strategy       = "imag_single",
  encoding_modality       = "visual",
  encoding_depth          = "shallow",
  VVIQ_score              = sample(21:80, quarter_rows),
  true_recall_proportion  = rnorm(quarter_rows, mean = 0.50, sd = 0.1),
  false_recall_proportion = rnorm(quarter_rows, mean = 0.50, sd = 0.25)
)

# visual deep data
visual_deep_b <- data.frame(
  participant             = (half_rows + quarter_rows + 1):num_rows,
  encoding_strategy       = "imag_association",
  encoding_modality       = "visual",
  encoding_depth          = "deep",
  VVIQ_score              = sample(21:80, num_rows - half_rows - quarter_rows),
  true_recall_proportion  = rnorm(num_rows - half_rows - quarter_rows, mean = 0.50, sd = 0.1),
  false_recall_proportion = rnorm(num_rows - half_rows - quarter_rows, mean = 0.50, sd = 0.25)
)

# Get the visual part of the data set
visual_data_b <- rbind(visual_shallow_b, visual_deep_b)

# Get the full data set
toy_set_b <- rbind(verbal_data_b, visual_data_b)
```

   4a. Calculate and output in R the **mean** and **standard error** of memory performance for each of the two conditions. Call these `verbal_mean_b` and `verbal_se_b`, and `visual_mean_b` and `visual_se_b` (6 points). 

``` {r 4a}
# Sources:
  # 1) https://dplyr.tidyverse.org/reference/summarise.html
  # 2) https://dplyr.tidyverse.org/reference/filter.html

# Group by modality (visual vs verbal)
grouped_toy_set_b <- group_by(toy_set_b, encoding_modality)

# Get mean and se for visual and verbal group 
group_info_b <- summarise(
  grouped_toy_set_b, 
  mean = mean(true_recall_proportion), 
  se = sd(true_recall_proportion) / sqrt(n())
)

# Extract verbal values
verbal_mean_b <- group_info_b$mean[group_info_b$encoding_modality == "verbal"]
verbal_se_b   <- group_info_b$se[group_info_b$encoding_modality == "verbal"]

# Extract visual values
visual_mean_b <- group_info_b$mean[group_info_b$encoding_modality == "visual"]
visual_se_b   <- group_info_b$se[group_info_b$encoding_modality == "visual"]
```
  
  
  5. Now let’s see what these values are like for the actual data! Calculate and report the **mean** and **standard error** separately for the verbal and visual participants in participant_level_data. Call these `verbal_mean_real` and `verbal_se_real`, and `visual_mean_real` and `visual_se_real` (4 points). 

```{r 2.5}
# Sources:
  # 1) https://dplyr.tidyverse.org/reference/summarise.html
  # 2) https://dplyr.tidyverse.org/reference/filter.html

# Group by modality (visual vs verbal)
grouped_real_data <- group_by(participant_level_data, encoding_modality)

# Get mean and se for visual and verbal group 
group_info_real <- summarise(
  grouped_real_data, 
  mean = mean(true_recall_proportion),
  se = sd(true_recall_proportion) / sqrt(n())
)

# Extract verbal values
verbal_mean_real <- group_info_real$mean[group_info_real$encoding_modality == "verbal"]
verbal_se_real   <- group_info_real$se[group_info_real$encoding_modality == "verbal"]

# Extract visual values
visual_mean_real <- group_info_real$mean[group_info_real$encoding_modality == "visual"]
visual_se_real   <- group_info_real$se[group_info_real$encoding_modality == "visual"]
```

# Part 3: Cleaning the data

  6. Write a function, `find_outlier`, to identify **outlier** from a list of data. It has the following constraints:  
    1. take as input a list of numbers  
    2. output a list of **booleans** (True or False), indicating whether a score is an outlier
    3. you can use the formula discussed in class.  
    
``` {r 6}
# Sources
  # 1) https://www.w3schools.com/R/r_functions.asp
  # 2) https://www.w3schools.com/r/r_for_loop.asp
  # 3) https://www.scribbr.com/frequently-asked-questions/how-do-i-find-quartiles-in-r/

find_outlier = function(num_list){
  
  # Get quantiles and calculate IQR
  quartiles <- quantile(num_list, prob=c(.25,.5,.75), type=1)
  Q1 <- quartiles[1]
  Q3 <- quartiles[3]
  IQR <- Q3 - Q1
  
  # Find our outlier bounds
  lower_bound <- Q1 - (1.5 * IQR)
  upper_bound <- Q3 + (1.5 * IQR)
  
  # Prep a results list for our booleans
  results_list <- list()
  
  # Iterate and construct the outlier boolean list
  for (data_point in num_list) {
    
    # If point falls outside of non-outlier range, then set value to TRUE
    if (data_point < lower_bound | data_point > upper_bound) {
      results_list <- c(results_list, TRUE)
    }
    # Otherwise, participant is not an outlier so set to FALSE
    else {
      results_list <- c(results_list, FALSE)
    }
  }
  
  # Return the list
  return(results_list)
}
```
  
   6a. Apply the function to **false_recall_proportion**, and then list the rows (participants) who were identified as outliers. These are participants who had many more (or fewer) false memories than expected—they are particularly sensitive to the DRM effect (4 points).  

```{r 6a}
# Sources
  # 1) https://codesignal.com/learn/courses/data-manipulation-in-r/lessons/exploring-data-frames-with-boolean-selection-in-r

# Extract the false recall column
false_recall_real <- participant_level_data$false_recall_proportion

# Get boolean list for which participants are outliers
false_recall_outliers_mask <- find_outlier(false_recall_real)

# Subset the participant df to get which ones are outliers
false_recall_outliers <- participant_level_data[unlist(false_recall_outliers_mask), ]

# Print the data
print(false_recall_outliers)
```
  
  6b. Apply the function to **vviq**, and then list the rows (participants) who were identified as outliers. These may be participants with aphantasia (a lack of visual imagery) or hyperphantasia (incredibly strong imagery)! (4 points).  

```{r 6b}
# Sources
  # 1) https://codesignal.com/learn/courses/data-manipulation-in-r/lessons/exploring-data-frames-with-boolean-selection-in-r

# Extract the false recall column
vviq_real <- participant_level_data$VVIQ_score

# Get boolean list for which participants are outliers
vviq_outliers_mask <- find_outlier(vviq_real)

# Subset the participant df to get which ones are outliers
vviq_outliers <- participant_level_data[unlist(vviq_outliers_mask), ]

# Print the data
print(vviq_outliers)
```

# Part 4: Plotting the data
  
  7. For each of `toy_set_a`, `toy_set_b`, and `class_data`, plot the following:  
    a. a **histogram** of true recall proportion across all data (2 point per plot).   
    b. a **scatterplot** of the true recall proportion for the two broader strategy conditions (verbal and visual). (2 point per plot).   
    c. a **bar graph** of the true recall proportion, with one bar for each of the two broader strategy conditions (verbal and visual). **include error bars indicating standard error** (2 point per plot).  
  
```{r 7}
# Sources
  # 1) https://www.datamentor.io/r-programming/histogram
  # 2) https://www.datacamp.com/doc/r/scatterplot-in-r
  # 3) https://r-graph-gallery.com/4-barplot-with-error-bar.html

# Get true recall proportion data for each data set
true_recall_graph_data_a <- toy_set_a$true_recall_proportion
true_recall_graph_data_b <- toy_set_b$true_recall_proportion
true_recall_graph_data_real <- participant_level_data$true_recall_proportion


# HISTOGRAMS
# ================================

# Toy Data A
# -------------------
hist(true_recall_graph_data_a,
     main = "Frequency of True Recall Proportions (Toy Set A)", 
     xlab = "True Recall Proportion", 
     xlim = c(0,1),
     col  = "red",
     )
# -------------------

# Toy Data B
# -------------------
hist(true_recall_graph_data_b,
     main = "Frequency of True Recall Proportions (Toy Set B)", 
     xlab = "True Recall Proportion", 
     xlim = c(0,1),
     col  = "blue",
     ) 
# -------------------

# Actual Class Data
# -------------------
hist(true_recall_graph_data_real,
     main = "Frequency of True Recall Proportions (Actual Data)", 
     xlab = "True Recall Proportion", 
     xlim = c(0,1),
     col  = "orange",
    )
# -------------------
# ================================

# SCATTERPLOTS
# ================================

# Toy Data A
# -------------------
x_vals_a <- ifelse(toy_set_a$encoding_modality == "verbal", 0.9, 1.1)
plot(
     x_vals_a,
     toy_set_a$true_recall_proportion,
     col = ifelse(toy_set_a$encoding_modality == "verbal", "orange", "purple"),
     pch = 16,
     xaxt = "n",
     xlab = "Encoding Modality",
     ylab = "True Recall Proportion",
     xlim = c(0.8, 1.2),
     ylim = c(0, 1),
     main = "True Recall Proportion by Modality (Toy Set A)"
)
axis(1, at = c(0.9, 1.1), labels = c("Verbal", "Visual"))
# -------------------

# Toy Data B
# -------------------
x_vals_b <- ifelse(toy_set_b$encoding_modality == "verbal", 0.9, 1.1)
plot(
     x_vals_b,
     toy_set_b$true_recall_proportion,
     col = ifelse(toy_set_b$encoding_modality == "verbal", "orange", "purple"),
     pch = 16,
     xaxt = "n",
     xlab = "Encoding Modality",
     ylab = "True Recall Proportion",
     xlim = c(0.8, 1.2),
     ylim = c(0, 1),
     main = "True Recall Proportion by Modality (Toy Set B)"
)
axis(1, at = c(0.9, 1.1), labels = c("Verbal", "Visual"))
# -------------------

# Actual Class Data
# -------------------
x_vals_real <- ifelse(participant_level_data$encoding_modality == "verbal", 0.9, 1.1)
plot(
     x_vals_real,
     participant_level_data$true_recall_proportion,
     col = ifelse(participant_level_data$encoding_modality == "verbal", "orange", "purple"),
     pch = 16,
     xaxt = "n",
     xlab = "Encoding Modality",
     ylab = "True Recall Proportion",
     xlim = c(0.8, 1.2),
     ylim = c(0, 1),
     main = "True Recall Proportion by Modality (Actual Data)"
)
axis(1, at = c(0.9, 1.1), labels = c("Verbal", "Visual"))
# -------------------
# ================================

# BAR GRAPHS
# ================================

# Toy Data A
# -------------------
# Combine group means and standard errors
means_a <- c(verbal_mean_a, visual_mean_a)
ses_a   <- c(verbal_se_a, visual_se_a)

# Create bar plot
bar_positions_a <- barplot(
  means_a,
  names.arg = c("Verbal", "Visual"),
  ylim = c(0, 1),
  col = c("orange", "purple"),
  ylab = "Mean True Recall Proportion",
  main = "True Recall Proportion by Modality (Toy Set A)"
)

# Add error bars
arrows(
  x0 = bar_positions_a,
  y0 = means_a - ses_a,
  x1 = bar_positions_a,
  y1 = means_a + ses_a,
  angle = 90,
  code = 3,
  length = 0.05
)
# -------------------

# Toy Data B
# -------------------
# Combine group means and standard errors
means_b <- c(verbal_mean_b, visual_mean_b)
ses_b   <- c(verbal_se_b, visual_se_b)

# Create the bar plot
bar_positions_b <- barplot(
  means_b,
  names.arg = c("Verbal", "Visual"),
  ylim = c(0, 1),
  col = c("orange", "purple"),
  ylab = "Mean True Recall Proportion",
  main = "True Recall Proportion by Modality (Toy Set B)"
)

# Add standard error bars
arrows(
  x0 = bar_positions_b,
  y0 = means_b - ses_b,
  x1 = bar_positions_b,
  y1 = means_b + ses_b,
  angle = 90,
  code = 3,
  length = 0.05
)
# -------------------

# Actual Class Data
# -------------------
# Combine group means and standard errors
means_real <- c(verbal_mean_real, visual_mean_real)
ses_real   <- c(verbal_se_real, visual_se_real)

# Create the bar plot
bar_positions_real <- barplot(
  means_real,
  names.arg = c("Verbal", "Visual"),
  ylim = c(0, 1),
  col = c("orange", "purple"),
  ylab = "Mean True Recall Proportion",
  main = "True Recall Proportion by Modality (Actual Data)"
)

# Add standard error bars
arrows(
  x0 = bar_positions_real,
  y0 = means_real - ses_real,
  x1 = bar_positions_real,
  y1 = means_real + ses_real,
  angle = 90,
  code = 3,
  length = 0.05
)
# -------------------
```

  8. **Describe:** Eye-balling the real data graphs and comparing them to the graphs for your toy datasets, would you guess there is a difference in true recall proportion for these two sets of participants? Why?  
    
  Write your answers below:  



  
Finally, **save** your toy datasets and the experimental data - we will use these in later assignments!  
``` {r save}

```
